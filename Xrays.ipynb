{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979ad769",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd3516",
   "metadata": {},
   "source": [
    "The business problem is a radiologist who wants to double-check their work with a model that classifies x-rays as Pneumonia or normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aad44e",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0b042",
   "metadata": {},
   "source": [
    "The data consists of 5,856 chest x-ray images. Each image is labelled as either normal or pneumonia.  25% of the images are labelled normal and 75% pneumonia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47dd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49c226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945e9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=Path('./data/archive/chest_xray/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b29a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d21cf",
   "metadata": {},
   "source": [
    "Loading and converting data to array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c68a5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#DONT RUN\n",
    "train_data_dir = i\n",
    "\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=5216, color_mode = \"grayscale\")\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f539ed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216, 64, 64, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c26c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#DONT RUN\n",
    "\n",
    "\n",
    "# Directory path\n",
    "val_data_dir = './data/archive/chest_xray/val'\n",
    "test_data_dir = './data/archive/chest_xray/test'\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "        val_data_dir, \n",
    "        target_size=(64, 64), batch_size= 16, color_mode = \"grayscale\")\n",
    "\n",
    "# Get all the data in the directory data/validation (132 images), and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=234+390, color_mode = \"grayscale\")\n",
    "\n",
    "\n",
    "# Create the datasets\n",
    "val_images, val_labels = next(val_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8156512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOS0uMLYz5i3tJGFB+X5TuU55BPQEcEHnrjIOvYafvhMiIcqecV0MVqrBZGykjnBXGMn1q8kP8adAACT1Xio3Z3RgJiuc+5xXPalA90x2lljVQoGew9aw7yyP9mxzBB8nyMR39D/AC/OuUvI/KkMqdV++P7wrrLSZbXT4yRln5xnr/kV1ukgtaELwzGnX880V5DGqqsUWfvHG4561vQJ+6Vm5crwPXFYF/qcllfRKiLIBy6+1WriKI2kjKu1XUMDnkA1z6TQmF7ObGGB4Pc/400+G9Gmto7GPzvtk4TNzIo8oSMjmNAwyVJZgDxg+X0zXLXF1vn3dB0X869D8P8AzRK3YKTz6jpTbvyrj5t5LDqfXtketdFbf8eg2n5dmN1cPqcW3U2cMynaPmPQda3UDvoyPJjccrx3HBz+tcNqF0sV7sflS2D9DWpY6lILieISFJ4kBifAIcrgrkHg8gH8Ae3PFXkuJ4FHOZVHX3r1LRwF0SR2Yr+6wCOoyR/n8an00G5iQmMLs4wR1H+c108ES/Z84wSPu54rk/EVvIu2VY8rnawVecVLpi+boUmGDBSCvqOuf6fnXl+uXOzXWjBAGT+Y4H9atWcr7cJIFkAVRlQeAR0/L8vxrAaQz38TD7vmKF/MV7BAGi8NLtUsxwVHqQD/AI0/QFJT5pwZGySRk9K7KL/Uj5ecYBrF1i0We2kUvh+o9j25rJ0fm2uo9uCF3HHQ8jB/nXkviJV/4SO8UdVIB/EZ/rUdnfGJ1Epxjo4/rUNpCZtQtowcHzAfy5/pXr1+I49HtLUuEIXLgck57fkBUOjzJDeQrsfbu28t1zXe26nyPvcYznHSs+ZRcBkVS3bg1jWNnJZX0kTkmOcN5Rx69uPwryLxJA0Pia834+dgw+mAP6VQCA1qeFLb7V4mt0aPcq8k+nI/pmvRVtJtU1GaY52k/LnkD0H4VvWmmLaspABYc8qOK3olQRgZP41nqJVmkUxhgwIx359K5/z73Sbxr5rOZbZiSySKyrjHYk4z7+nSuE+I2n/ZPEUV2AfLu0JQkY6YPI6g/OPTjB7muYTnFdJ8PYi2uSTcFVXp7gf/AF69ds7eOCMYX8KtsCcY49hVrgAHb29KhikkhLhWIVwQfcVS1W2e6gIb94QCBu5K/TP0HHT2rzD4kvKV0oSqyFVIVCwIVTyAAAAD68c8VxadBX//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(train_images[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4d0728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1583., 4273.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels1)+ sum(test_labels1)+sum(val_labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629464a",
   "metadata": {},
   "source": [
    "Training class distribution is 1583 to 4273, i.e. 27% and 73%, normal and pneumonia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581df4a",
   "metadata": {},
   "source": [
    "Pickling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94911bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e57b7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_images.pickle', 'wb') as f:\n",
    "    pickle.dump(train_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc338a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f6a5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2b45cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_images.pickle', 'wb') as f:\n",
    "    pickle.dump(test_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "157ea59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e714122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_images.pickle', 'wb') as f:\n",
    "    pickle.dump(val_images, f)\n",
    "\n",
    "    \n",
    "with open('val_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(val_labels, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ab18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_images.pickle', 'rb') as g:\n",
    "    train_images2= pickle.load(g)\n",
    "with open('test_images.pickle', 'rb') as f:\n",
    "    test_images1= pickle.load(f)\n",
    "with open('val_images.pickle', 'rb') as f:\n",
    "    val_images1= pickle.load(f)\n",
    "    \n",
    "with open('train_labels.pickle', 'rb') as f:\n",
    "    train_labels1= pickle.load(f)\n",
    "with open('test_labels.pickle', 'rb') as f:\n",
    "    test_labels1= pickle.load(f)\n",
    "with open('val_labels.pickle', 'rb') as f:\n",
    "    val_labels1= pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c70c0d",
   "metadata": {},
   "source": [
    "Combining data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfae61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.append(train_images2, val_images1, axis=0)\n",
    "t1 = np.append(train_labels1, val_labels1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d19e7d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 64, 64, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.append(t, test_images1, axis=0)\n",
    "t1 = np.append(t1, test_labels1, axis=0)\n",
    "np.shape(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979ef6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.shape(t1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfea732",
   "metadata": {},
   "source": [
    "Splitting data into training, validation and test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d21b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(t, t1, random_state =5) \n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, random_state =5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd78d5",
   "metadata": {},
   "source": [
    "Reshaping data for modelling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47ce2685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 4096)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = np.reshape(test_images, (1464,64*64))\n",
    "np.shape(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188f4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.reshape(train_images, (np.shape(train_images)[0],64*64))\n",
    "np.shape(train_images)\n",
    "\n",
    "val_images = np.reshape(val_images, (np.shape(val_images)[0],64*64))\n",
    "#np.shape(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f93439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#s = pd.Series(val)\n",
    "#s.sort_values(desc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780d7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1f0d9",
   "metadata": {},
   "source": [
    "Removing extra column from labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de40b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= train_labels[:,0]\n",
    "val_labels= val_labels[:,0]\n",
    "test_labels= test_labels[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00593294",
   "metadata": {},
   "source": [
    "Standardizing image data by diving by the maximum pixel value of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bad158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b334ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images=val_images/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063cb0f7",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889b3d5",
   "metadata": {},
   "source": [
    "Building intial model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d70db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec1047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b437a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 1s 7ms/step - loss: 0.1824 - acc: 0.7310 - val_loss: 0.1562 - val_acc: 0.7641\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1526 - acc: 0.7872 - val_loss: 0.1348 - val_acc: 0.8097\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1347 - acc: 0.8297 - val_loss: 0.1226 - val_acc: 0.8579\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1234 - acc: 0.8503 - val_loss: 0.1306 - val_acc: 0.8579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cc33b36b80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_labels, batch_size=64, epochs=4, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "144c9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6d539",
   "metadata": {},
   "source": [
    "Builing second model by adding an additional layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672e58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(32, activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39151cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "094710f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 0.1889 - acc: 0.7231 - val_loss: 0.1683 - val_acc: 0.7441\n",
      "Epoch 2/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1633 - acc: 0.7523 - val_loss: 0.1501 - val_acc: 0.7541\n",
      "Epoch 3/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1437 - acc: 0.8090 - val_loss: 0.1360 - val_acc: 0.7842\n",
      "Epoch 4/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1297 - acc: 0.8412 - val_loss: 0.1196 - val_acc: 0.8543\n",
      "Epoch 5/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1214 - acc: 0.8567 - val_loss: 0.1170 - val_acc: 0.8807\n",
      "Epoch 6/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1126 - acc: 0.8676 - val_loss: 0.1069 - val_acc: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cc33fd2f10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images,train_labels, batch_size=64, epochs=6, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4875d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model2.pickle', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7d654",
   "metadata": {},
   "source": [
    "Model 2 achieves a higher validation accuracy of 89% as opposed to baseline model's 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e9951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257787ed",
   "metadata": {},
   "source": [
    "Building third model by increasing the learning rate of the optimizer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5e1b8",
   "metadata": {},
   "source": [
    "Use same layers as model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "767f7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = .05)\n",
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model3.add(tf.keras.layers.Dense(32, activation = 'tanh'))\n",
    "model3.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce118ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer = opt, loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e023111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 0.1859 - acc: 0.7641 - val_loss: 0.1439 - val_acc: 0.7559\n",
      "Epoch 2/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1429 - acc: 0.8103 - val_loss: 0.1400 - val_acc: 0.7933\n",
      "Epoch 3/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1367 - acc: 0.8127 - val_loss: 0.1194 - val_acc: 0.8206\n",
      "Epoch 4/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1053 - acc: 0.8588 - val_loss: 0.1287 - val_acc: 0.8188\n",
      "Epoch 5/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1119 - acc: 0.8464 - val_loss: 0.0951 - val_acc: 0.8716\n",
      "Epoch 6/6\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1104 - acc: 0.8531 - val_loss: 0.0963 - val_acc: 0.8743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cc40d29940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_images,train_labels, batch_size=64, epochs=6, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a312d",
   "metadata": {},
   "source": [
    "Appears to be over-learning, minimum loss is overshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ee42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=test_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0261335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step - loss: 0.0863 - acc: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08634314686059952, 0.8845628499984741]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd708a5e",
   "metadata": {},
   "source": [
    "Accuracy on test set is 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a286cb",
   "metadata": {},
   "source": [
    "# Results/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed37f0",
   "metadata": {},
   "source": [
    "The 2nd model performed best based on the validation sets, higher accuracy than the baseline model and the faster learning model.  The chosen model's accuracy on the holdout test set was 88%.  This can be compared to guessing based on the sample balance, which would yield 73% accuracy.  Thus this model can be used as a check by the radiologist, for instance on x-rays that they are less certain about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa65a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dash-env)",
   "language": "python",
   "name": "dash-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
