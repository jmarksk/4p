{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48f613c",
   "metadata": {},
   "source": [
    "# Xray classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ad769",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd3516",
   "metadata": {},
   "source": [
    "The business problem is a radiologist who wants to double-check their work with a model that classifies x-rays as Pneumonia or normal.  The practice has supplied us with their x-rays that they have classified and has asked us to build a model which they can refer to on difficult to classify images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aad44e",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0b042",
   "metadata": {},
   "source": [
    "The data consists of 5,856 chest x-ray images. Each image is labelled as either normal or pneumonia.  25% of the images are labelled normal and 75% pneumonia.  The data comes from an x-ray imaging lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f47dd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b49c226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "945e9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=Path('./data/archive/chest_xray/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3c5ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6b29a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d21cf",
   "metadata": {},
   "source": [
    "Loading and converting data to array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bfa0708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data uploading\n",
    "#train_data_dir = i\n",
    "\n",
    "#Get all the data in the directory data/train, and reshape them\n",
    "#train_generator = ImageDataGenerator().flow_from_directory(\n",
    " #train_data_dir, \n",
    "   #target_size=(64, 64), batch_size=5216, color_mode = \"grayscale\")\n",
    "\n",
    "#Create the datasets\n",
    "#train_images, train_labels = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5b100187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data uploading\n",
    "\n",
    "\n",
    "#Directory path\n",
    "#val_data_dir = './data/archive/chest_xray/val'\n",
    "#test_data_dir = './data/archive/chest_xray/test'\n",
    "\n",
    "#Get all the data in the directory data/train (790 images), and reshape them\n",
    "#val_generator = ImageDataGenerator().flow_from_directory(\n",
    "        #val_data_dir, \n",
    "        #target_size=(64, 64), batch_size= 16, color_mode = \"grayscale\")\n",
    "\n",
    "#Get all the data in the directory data/validation (132 images), and reshape them\n",
    "#test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        #test_data_dir, \n",
    "        #target_size=(64, 64), batch_size=234+390, color_mode = \"grayscale\")\n",
    "\n",
    "\n",
    "#Create the datasets\n",
    "#val_images, val_labels = next(val_generator)\n",
    "#test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581df4a",
   "metadata": {},
   "source": [
    "Pickling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "94911bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc338a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d892b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('train_images.pickle', 'wb') as f:\n",
    "    #pickle.dump(train_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7f6a5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('train_labels.pickle', 'wb') as f:\n",
    "    #pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d2b45cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('test_images.pickle', 'wb') as f:\n",
    "    #pickle.dump(test_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "157ea59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('test_labels.pickle', 'wb') as f:\n",
    "    #pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e714122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('val_images.pickle', 'wb') as f:\n",
    "    #pickle.dump(val_images, f)\n",
    "\n",
    "    \n",
    "#with open('val_labels.pickle', 'wb') as f:\n",
    "    #pickle.dump(val_labels, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b0ab18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_images.pickle', 'rb') as g:\n",
    "    train_images= pickle.load(g)\n",
    "with open('test_images.pickle', 'rb') as f:\n",
    "    test_images= pickle.load(f)\n",
    "with open('val_images.pickle', 'rb') as f:\n",
    "    val_images= pickle.load(f)\n",
    "    \n",
    "with open('train_labels.pickle', 'rb') as f:\n",
    "    train_labels= pickle.load(f)\n",
    "with open('test_labels.pickle', 'rb') as f:\n",
    "    test_labels= pickle.load(f)\n",
    "with open('val_labels.pickle', 'rb') as f:\n",
    "    val_labels= pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4768aef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1583., 4273.], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels)+ sum(test_labels)+sum(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65688f2",
   "metadata": {},
   "source": [
    "Class distribution is 1583 to 4273, i.e. 27% and 73%, normal and pneumonia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0a81bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf49c2",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e445bf",
   "metadata": {},
   "source": [
    "![scan](./Images/scan.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c70c0d",
   "metadata": {},
   "source": [
    "Combining data from train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "bfae61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.append(train_images, test_images, axis=0)\n",
    "t1 = np.append(train_labels, test_labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6f3916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfea732",
   "metadata": {},
   "source": [
    "Splitting combined data into training and test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c8d21b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(t, t1, test_size = .25, random_state =5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd78d5",
   "metadata": {},
   "source": [
    "Reshaping data for modelling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "47ce2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.reshape(test_images, (np.shape(test_images)[0],64*64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "188f4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.reshape(train_images, (np.shape(train_images)[0],64*64))\n",
    "np.shape(train_images)\n",
    "\n",
    "val_images = np.reshape(val_images, (np.shape(val_images)[0],64*64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f663986",
   "metadata": {},
   "source": [
    "Removing extra column from labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "634bfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= train_labels[:,0]\n",
    "val_labels= val_labels[:,0]\n",
    "test_labels= test_labels[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffab15",
   "metadata": {},
   "source": [
    "Standardizing image data by diving by the maximum pixel value of 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "419588c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "93ee940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images=val_images/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83fa1a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6f93439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "780d7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063cb0f7",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889b3d5",
   "metadata": {},
   "source": [
    "Building initial model, a neural network with 1 hidden layer, with 64 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4d70db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ec1047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0b437a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "69/69 [==============================] - 1s 5ms/step - loss: 0.1778 - acc: 0.7322 - val_loss: 0.3191 - val_acc: 0.5000\n",
      "Epoch 2/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1387 - acc: 0.8174 - val_loss: 0.2179 - val_acc: 0.6875\n",
      "Epoch 3/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1185 - acc: 0.8607 - val_loss: 0.1682 - val_acc: 0.8125\n",
      "Epoch 4/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1097 - acc: 0.8685 - val_loss: 0.1661 - val_acc: 0.8125\n",
      "Epoch 5/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1016 - acc: 0.8822 - val_loss: 0.2055 - val_acc: 0.6875\n",
      "Epoch 6/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0974 - acc: 0.8831 - val_loss: 0.1954 - val_acc: 0.6875\n",
      "Epoch 7/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0933 - acc: 0.8854 - val_loss: 0.1672 - val_acc: 0.7500\n",
      "Epoch 8/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0888 - acc: 0.8945 - val_loss: 0.1745 - val_acc: 0.6875\n",
      "Epoch 9/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0881 - acc: 0.8952 - val_loss: 0.1254 - val_acc: 0.9375\n",
      "Epoch 10/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0852 - acc: 0.8995 - val_loss: 0.1204 - val_acc: 0.9375\n",
      "Epoch 11/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0829 - acc: 0.8998 - val_loss: 0.1444 - val_acc: 0.8125\n",
      "Epoch 12/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0815 - acc: 0.9014 - val_loss: 0.1547 - val_acc: 0.7500\n",
      "Epoch 13/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0811 - acc: 0.9023 - val_loss: 0.1108 - val_acc: 0.9375\n",
      "Epoch 14/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0787 - acc: 0.9059 - val_loss: 0.1085 - val_acc: 0.9375\n",
      "Epoch 15/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0774 - acc: 0.9071 - val_loss: 0.1170 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x258347e0b80>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_labels, batch_size=64, epochs=15, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "469f4df0",
   "metadata": {},
   "source": [
    "88% and 90% validation and training accuracies, suggests possible overfitting. Next model reduces the complexity of the model in order to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ab6bb",
   "metadata": {},
   "source": [
    "Builing 1a model with 32 neurons and same tanh activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "78f41b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1a = tf.keras.Sequential()\n",
    "model1a.add(tf.keras.layers.Dense(32, input_shape = (4096,), activation = 'tanh'))\n",
    "\n",
    "model1a.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "59faca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1a.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "415407dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "69/69 [==============================] - 1s 4ms/step - loss: 0.1770 - acc: 0.7486 - val_loss: 0.2074 - val_acc: 0.6875\n",
      "Epoch 2/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1331 - acc: 0.8265 - val_loss: 0.2422 - val_acc: 0.5625\n",
      "Epoch 3/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1164 - acc: 0.8546 - val_loss: 0.1736 - val_acc: 0.8125\n",
      "Epoch 4/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1028 - acc: 0.8790 - val_loss: 0.1420 - val_acc: 0.7500\n",
      "Epoch 5/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0964 - acc: 0.8861 - val_loss: 0.1570 - val_acc: 0.8125\n",
      "Epoch 6/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0936 - acc: 0.8852 - val_loss: 0.1463 - val_acc: 0.8125\n",
      "Epoch 7/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0922 - acc: 0.8888 - val_loss: 0.1468 - val_acc: 0.8125\n",
      "Epoch 8/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0865 - acc: 0.8984 - val_loss: 0.1351 - val_acc: 0.8125\n",
      "Epoch 9/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0846 - acc: 0.8977 - val_loss: 0.1599 - val_acc: 0.8125\n",
      "Epoch 10/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0822 - acc: 0.9009 - val_loss: 0.1113 - val_acc: 0.8750\n",
      "Epoch 11/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0821 - acc: 0.8984 - val_loss: 0.1141 - val_acc: 0.8750\n",
      "Epoch 12/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0781 - acc: 0.9066 - val_loss: 0.1467 - val_acc: 0.8125\n",
      "Epoch 13/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0775 - acc: 0.9057 - val_loss: 0.1415 - val_acc: 0.8125\n",
      "Epoch 14/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0772 - acc: 0.9057 - val_loss: 0.1505 - val_acc: 0.8125\n",
      "Epoch 15/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0778 - acc: 0.9064 - val_loss: 0.0954 - val_acc: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25834882fa0>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1a.fit(train_images,train_labels, batch_size=64, epochs=15, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897a3e2",
   "metadata": {},
   "source": [
    "Same training acc of 90% but val acc hovering higher at 90% (88% in baseline), suggests overfitting reduced. Next model increases the complexity of the model in order to increase accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6d539",
   "metadata": {},
   "source": [
    "Builing second model by adding to baseline an additional layer with 32 neurons and same tanh activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b3008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "672e58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(32, activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "39151cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "094710f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "69/69 [==============================] - 1s 5ms/step - loss: 0.1767 - acc: 0.7452 - val_loss: 0.3544 - val_acc: 0.5000\n",
      "Epoch 2/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1324 - acc: 0.8281 - val_loss: 0.2459 - val_acc: 0.5625\n",
      "Epoch 3/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1149 - acc: 0.8557 - val_loss: 0.2048 - val_acc: 0.6875\n",
      "Epoch 4/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1017 - acc: 0.8719 - val_loss: 0.3107 - val_acc: 0.5000\n",
      "Epoch 5/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0958 - acc: 0.8826 - val_loss: 0.1792 - val_acc: 0.7500\n",
      "Epoch 6/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0936 - acc: 0.8811 - val_loss: 0.1059 - val_acc: 0.9375\n",
      "Epoch 7/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0886 - acc: 0.8897 - val_loss: 0.1922 - val_acc: 0.6875\n",
      "Epoch 8/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0849 - acc: 0.8947 - val_loss: 0.1141 - val_acc: 0.8750\n",
      "Epoch 9/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0828 - acc: 0.8991 - val_loss: 0.0922 - val_acc: 0.9375\n",
      "Epoch 10/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0798 - acc: 0.9018 - val_loss: 0.1038 - val_acc: 0.9375\n",
      "Epoch 11/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0785 - acc: 0.9023 - val_loss: 0.0924 - val_acc: 0.9375\n",
      "Epoch 12/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0763 - acc: 0.9066 - val_loss: 0.0845 - val_acc: 0.9375\n",
      "Epoch 13/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0755 - acc: 0.9064 - val_loss: 0.0822 - val_acc: 0.9375\n",
      "Epoch 14/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0749 - acc: 0.9089 - val_loss: 0.0837 - val_acc: 0.9375\n",
      "Epoch 15/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0749 - acc: 0.9075 - val_loss: 0.1784 - val_acc: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x258357050d0>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images,train_labels, batch_size=64, epochs=15, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7d654",
   "metadata": {},
   "source": [
    "Model 2 validation accuracy is reduced to 81% as opposed to baseline model's 88% and model 1a's 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257787ed",
   "metadata": {},
   "source": [
    "Building third model by increasing the learning rate of the optimizer from default .001 to .05 in order to try to increase accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5e1b8",
   "metadata": {},
   "source": [
    "Use same layers as model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "767f7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = .05)\n",
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model3.add(tf.keras.layers.Dense(32, activation = 'tanh'))\n",
    "model3.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ce118ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer = opt, loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9e023111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "69/69 [==============================] - 1s 5ms/step - loss: 0.1905 - acc: 0.7358 - val_loss: 0.3323 - val_acc: 0.5000\n",
      "Epoch 2/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1544 - acc: 0.7813 - val_loss: 0.2210 - val_acc: 0.6875\n",
      "Epoch 3/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1318 - acc: 0.8237 - val_loss: 0.1535 - val_acc: 0.8125\n",
      "Epoch 4/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1144 - acc: 0.8473 - val_loss: 0.1372 - val_acc: 0.7500\n",
      "Epoch 5/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1040 - acc: 0.8630 - val_loss: 0.1167 - val_acc: 0.8125\n",
      "Epoch 6/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0952 - acc: 0.8747 - val_loss: 0.1375 - val_acc: 0.8125\n",
      "Epoch 7/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0939 - acc: 0.8779 - val_loss: 0.1065 - val_acc: 0.8750\n",
      "Epoch 8/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0868 - acc: 0.8854 - val_loss: 0.0981 - val_acc: 0.8750\n",
      "Epoch 9/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0818 - acc: 0.8947 - val_loss: 0.0977 - val_acc: 0.8750\n",
      "Epoch 10/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0787 - acc: 0.8973 - val_loss: 0.1269 - val_acc: 0.8125\n",
      "Epoch 11/15\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0835 - acc: 0.8909 - val_loss: 0.0825 - val_acc: 0.9375\n",
      "Epoch 12/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0820 - acc: 0.8973 - val_loss: 0.0820 - val_acc: 0.8750\n",
      "Epoch 13/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0783 - acc: 0.9011 - val_loss: 0.1637 - val_acc: 0.7500\n",
      "Epoch 14/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0762 - acc: 0.9023 - val_loss: 0.1145 - val_acc: 0.8750\n",
      "Epoch 15/15\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0771 - acc: 0.9007 - val_loss: 0.1387 - val_acc: 0.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25834dbfdc0>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_images,train_labels, batch_size=64, epochs=15, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a312d",
   "metadata": {},
   "source": [
    "Appears to be over-learning, minimum loss is overshot.  Model1a has best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc504b",
   "metadata": {},
   "source": [
    "![acctable](./Images/acctable.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3093773",
   "metadata": {},
   "source": [
    "Evaluate model 1a on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "469c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=test_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f0261335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step - loss: 0.0890 - acc: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08899200707674026, 0.8869863152503967]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1a.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd708a5e",
   "metadata": {},
   "source": [
    "Accuracy on test set is 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f67b7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model1a.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fd53e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat= []\n",
    "for i in preds:\n",
    "    if i<.5:\n",
    "        yhat.append(0)\n",
    "    else:\n",
    "        yhat.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "39493afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d04e6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = confusion_matrix(test_labels, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e9b65d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[982,  98],\n",
       "       [ 67, 313]], dtype=int64)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "82cd951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a0e198b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x258356f81c0>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAGwCAYAAADMjZ3mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3lUlEQVR4nO3deXgUVdbH8V8nIfsCwSEhEDZRIIIgoBg3ZIwEQYWBVwcnSkTAEUFZRhBG2VWUEUEQQVFZZkDFjZFFNIKASNiJwxpFwIQlQY0hJJCtu94/kJa2KU3TSTqhv5/nqeeZrr63+rQTzck5t25ZDMMwBAAAcB4fTwcAAACqHhIEAADghAQBAAA4IUEAAABOSBAAAIATEgQAAOCEBAEAADjx83QAlc1ms+nYsWMKCwuTxWLxdDgAABcZhqFTp04pJiZGPj4V93duYWGhiouL3b6Ov7+/AgMDyyGiyuV1CcKxY8cUGxvr6TAAAG7KzMxU/fr1K+TahYWFatwwVFknrG5fKzo6WocOHap2SYLXJQhhYWGSpO93NFJ4KB0WXJp6XdPB0yEAFabUKNH60+/b/3teEYqLi5V1wqrvtzdSeNjF/67IO2VTw3aHVVxcTIJQ1Z1rK4SH+rj1fzpQlflZ/D0dAlDhKqNNHBpmUWjYxX+OTdW3le11CQIAAGVlNWyyuvHEIqthK79gKhkJAgAAJmwyZNPFZwjuzPU0auwAAMAJFQQAAEzYZJM7TQL3ZnsWCQIAACashiGrcfFtAnfmehotBgAA4IQKAgAAJrx5kSIJAgAAJmwyZPXSBIEWAwAAcEIFAQAAE7QYAACAE+5iAAAAOA8VBAAATNh+OdyZX12RIAAAYMLq5l0M7sz1NBIEAABMWA25+TTH8oulsrEGAQAAOKGCAACACdYgAAAAJzZZZJXFrfnVFS0GAADghAoCAAAmbMbZw5351RUJAgAAJqxuthjcmetptBgAAIATKggAAJjw5goCCQIAACZshkU2w427GNyY62m0GAAAgBMqCAAAmKDFAAAAnFjlI6sbxXZrOcZS2UgQAAAwYbi5BsFgDQIAALiUUEEAAMAEaxAAAIATq+Ejq+HGGoRqvNUyLQYAAOCECgIAACZsssjmxt/SNlXfEgIJAgAAJrx5DQItBgAA4IQKAgAAJtxfpEiLAQCAS87ZNQhuPKyJFgMAALiUUEEAAMCEzc1nMXAXAwAAlyDWIAAAACc2+XjtPgisQQAAAE6oIAAAYMJqWGR145HN7sz1NBIEAABMWN1cpGilxQAAAC4lVBAAADBhM3xkc+MuBht3MQAAcOmhxQAAAHAeKggAAJiwyb07EWzlF0qlI0EAAMCE+xslVd9CffWNHAAAVBgqCAAAmHD/WQzV9+9wEgQAAEzYZJFN7qxBYCdFAAAuOd5cQai+kQMAgApDBQEAABPub5RUff8OJ0EAAMCEzbDI5s4+CNX4aY7VN7UBAAAVhgoCAAAmbG62GKrzRkkkCAAAmHD/aY7VN0GovpEDAIAKQwUBAAATVllkdWOzI3fmehoJAgAAJmgxAAAAnIcKAgAAJqxyr01gLb9QKh0JAgAAJry5xUCCAACACR7WBAAAPM5qtWrMmDFq3LixgoKCdPnll2vSpEkyDMM+xjAMjR07VnXr1lVQUJASEhL07bffOlwnJydHSUlJCg8PV82aNdWvXz/l5+e7FAsJAgAAJgxZZHPjMFxcv/DCCy9o9uzZeuWVV7Rv3z698MILmjJlimbOnGkfM2XKFM2YMUNz5szR5s2bFRISosTERBUWFtrHJCUlac+ePUpJSdHy5cu1fv16Pfzwwy7FQosBAAATld1i2Lhxo7p3765u3bpJkho1aqS3335bW7ZskXS2ejB9+nQ9/fTT6t69uyRp4cKFioqK0tKlS9W7d2/t27dPq1at0tatW9W+fXtJ0syZM9W1a1e9+OKLiomJKVMsVBAAAKhgeXl5DkdRUdEFx91www1avXq1vvnmG0nS119/rQ0bNuiOO+6QJB06dEhZWVlKSEiwz4mIiFCHDh2UmpoqSUpNTVXNmjXtyYEkJSQkyMfHR5s3by5zzFQQAAAwUV6Pe46NjXU4P27cOI0fP95p/KhRo5SXl6fmzZvL19dXVqtVzz77rJKSkiRJWVlZkqSoqCiHeVFRUfb3srKyVKdOHYf3/fz8FBkZaR9TFiQIAACYsLr5NMdzczMzMxUeHm4/HxAQcMHxS5Ys0aJFi7R48WJdddVVSktL09ChQxUTE6Pk5OSLjuNikCAAAFDBwsPDHRIEMyNGjNCoUaPUu3dvSVKrVq30/fffa/LkyUpOTlZ0dLQkKTs7W3Xr1rXPy87OVps2bSRJ0dHROnHihMN1S0tLlZOTY59fFqxBAADAxLkWgzuHK06fPi0fH8dfzb6+vrLZbJKkxo0bKzo6WqtXr7a/n5eXp82bNys+Pl6SFB8fr9zcXG3fvt0+Zs2aNbLZbOrQoUOZY6GCAACACZt8ZHPjb2lX595111169tln1aBBA1111VXauXOnXnrpJT300EOSJIvFoqFDh+qZZ57RFVdcocaNG2vMmDGKiYlRjx49JEktWrRQly5dNGDAAM2ZM0clJSUaPHiwevfuXeY7GCQSBAAAqoyZM2dqzJgxevTRR3XixAnFxMTo73//u8aOHWsfM3LkSBUUFOjhhx9Wbm6ubrrpJq1atUqBgYH2MYsWLdLgwYN12223ycfHR7169dKMGTNcisVinL89kxfIy8tTRESEfv6micLD6LDg0nTHFTd6OgSgwpQaxVpT8LZOnjxZpr7+xTj3u2Lglz0VEFrjoq9TlF+i2Td/WKGxVhQqCAAAmCiv2xyrIxIEAABMGG4+zdHgYU0AAOBSQgUBAAATVllkdfGBS7+dX12RIAAAYMJmuLeOwFaNbwOgxQAAAJxQQcBFOZ3vowVT6mrjJxHK/clPl191RgMnHVGzNmckSWcKfPTms3WV+mmE8n72U3Rssbr3+0F39vlJkpT3s6/+/WK0dqwL04lj/oqILNUNXU4qeeRxhYTbPPnVgAsKCrGqz9AMxd/+k2rWLtV3e0P02jON9M2uMElSYLBVfZ/4XjfcnqOwmqXKPhKg/y6sq5Vvl31rW1Q9NjcXKboz19NIEHBRpv0jVofTAzVy5veKjCrRmg8iNeqvTTV37X5dVrdEr42PUdpXYRo5M0NRscXasS5MM0fXV+2oEsUn5iknu4Z+yq6hAWOPqcGVhTpxxF8zRtXXT9k1NGbuYU9/PcDJkGcPqNGVp/XiiCv0U7a//tz9Bz23YK/+fkcb/ZQdoIdHH1br+JOa8o8rlH00QO1uytWg8Qf1U7a/Nq+J9HT4uEg2WWRzYx2BO3M9rUqkNrNmzVKjRo0UGBioDh06aMuWLb87/r333lPz5s0VGBioVq1aaeXKlZUUKSSp6IxFG1bWVP+nj6vV9QWq17hYDzyRpZhGRVq+sLYkae+2EN1+T45a35Cv6Nhidb3/JzWJO6P0tGBJUqPmhRr7xmFd3zlPMY2K1eamfD345HFtTgmXtdST3w5w5h9g1U2JP+nNKQ21e2uEjmcEadHMBjr2faC6/S1bktSibZ4+/+hP2rUlQieOBuqTd6N1cH+ImrXO93D0wMXxeILw7rvvavjw4Ro3bpx27Nih1q1bKzEx0elJVOds3LhR9913n/r166edO3eqR48e6tGjh3bv3l3JkXsvq9Uim9Ui/wDHVkBAoE17toRKkuLaF2jTZxH68XgNGYaU9lWojh4MULuOp0yvW5Dnq+BQm3ypa6GK8fU7e5QUOf4ns7jQR1e1y5Mk7dsRruv/nKPaUUWSDF3d4aTqNTqjHRsiPBAxyovVsLh9VFceTxBeeuklDRgwQH379lVcXJzmzJmj4OBgvfXWWxcc//LLL6tLly4aMWKEWrRooUmTJqlt27Z65ZVXKjly7xUcalOLdgVaPD1aP2X5yWqVVn9QS/u2hygn++xv90efOaoGVxYqqd1V6tawtZ5OaqJBzx1Rq+sLLnjNkz/5avH0aN1x/4+V+VWAMjlT4Ku9O8J036AjiqxTLB8fQ53u/kHNrzmlyD8VS5JmT2qsjAPB+s+G7Vq2d5OeeWuvXp3QRLu3kiBUZ+fWILhzVFce/VutuLhY27dv1+jRo+3nfHx8lJCQoNTU1AvOSU1N1fDhwx3OJSYmaunSpRccX1RUpKKiIvvrvLw89wOHRs78Xi8Nb6C/tW0pH19DTVud1q09fta3/zvbQvjvW5dp//ZgTZh/UHXqF2vXplDN+ufZNQhtb3EsuRac8tGYPk3U4MpCPfCPLE98HeAPvTjiCg2bfECLvtoma6l0YE+o1i2/TE1bnk16737guJq3OaXxf2+u7KMBanVtnh4dd1A/nfBX2saang0euAgeTRB+/PFHWa1WRUVFOZyPiorS/v37LzgnKyvrguOzsi78i2Xy5MmaMGFC+QQMu5hGxXrxwwMqPO2jglM+qh1Vqmf/3lB1Gxap6IxF85+vq7FvHlaHhLMJWZO4Qh3cE6T359RxSBBO5/voqb9drqAQm8a9eUh+F/9MFKBCHc8I1MiklgoIsio41Kqff/DXqOnpysoMkH+AVcnDMzRpUDNtXXt2QeLh9BA1aVGgXv2OkSBUYza5+SwGFilWXaNHj9bJkyftR2ZmpqdDuqQEBttUO6pUp3J9tX1duOIT81RaalFpiY98fBx3CPHxNWSct2yh4JSP/nnf5arhb2jC/IPyD6zGO4rAaxSd8dXPP/grNLxU7W7O1abPI+VXw1ANf0OGzfGXgc1mcfr3ANWL8ctdDBd7GNU4QfBoBeGyyy6Tr6+vsrOzHc5nZ2crOvrC9w5HR0e7ND4gIEABAQHlEzDstq0Nk2FIsZcX6eghf70xqZ5imxaq819/kl8N6er4fM2dFCP/wKOKql+s/6WG6vP3I/XwuKOSfk0Ois74aOTMQzqd76vTvxQWImqXytfXg18OuIC2N/0si0U6cihIMQ0L1e/JwzpyMEiffVBH1lIf/W9zuPo9eVhFhT46cSxAra7L0209ftDcyY08HTrcwNMcPcTf31/t2rXT6tWr1aNHD0mSzWbT6tWrNXjw4AvOiY+P1+rVqzV06FD7uZSUFMXHx1dCxDinIM9X8ybX1Y/HayisplU3ds1V31HH7S2C0bMP663n6uqFwQ10KtdPdeoV68Enj9s3SjqwK1j7d4RIkvreEOdw7QWb9yo6trhSvw/wR0LCzm6EdFl0sU7l+mnDp7W14KUGspaeLcQ+P/RKPfjE9xo59VuF1SzViaMBWvBSA61YHPUHVwaqJo/fUDZ8+HAlJyerffv2uu666zR9+nQVFBSob9++kqQ+ffqoXr16mjx5siRpyJAh6tixo6ZOnapu3brpnXfe0bZt2/T666978mt4nY5356rj3bmm70fWKdUT083bOa1vyNenx9LKPzCggnz5yWX68pPLTN//+Ud/TRt1RSVGhMrATooe9Ne//lU//PCDxo4dq6ysLLVp00arVq2yL0TMyMiQj8+v/4BvuOEGLV68WE8//bT++c9/6oorrtDSpUvVsmVLT30FAMAlihaDhw0ePNi0pbB27Vqnc/fcc4/uueeeCo4KAADvVSUSBAAAqiJvfhYDCQIAACa8ucVQfVdPAACACkMFAQAAE95cQSBBAADAhDcnCLQYAACAEyoIAACY8OYKAgkCAAAmDLl3q2J1flQXCQIAACa8uYLAGgQAAOCECgIAACa8uYJAggAAgAlvThBoMQAAACdUEAAAMOHNFQQSBAAATBiGRYYbv+TdmetptBgAAIATKggAAJiwyeLWRknuzPU0EgQAAEx48xoEWgwAAMAJFQQAAEx48yJFEgQAAEx4c4uBBAEAABPeXEFgDQIAAHBCBQEAABOGmy2G6lxBIEEAAMCEIckw3JtfXdFiAAAATqggAABgwiaLLOykCAAAzsddDAAAAOehggAAgAmbYZGFjZIAAMD5DMPNuxiq8W0MtBgAAIATKggAAJjw5kWKJAgAAJggQQAAAE68eZEiaxAAAIATKggAAJjw5rsYSBAAADBxNkFwZw1COQZTyWgxAAAAJ1QQAAAwwV0MAADAifHL4c786ooWAwAAcEIFAQAAE7QYAACAMy/uMZAgAABgxs0KgqpxBYE1CAAAwAkVBAAATLCTIgAAcOLNixRpMQAAACdUEAAAMGNY3FtoWI0rCCQIAACY8OY1CLQYAACoQo4ePar7779ftWvXVlBQkFq1aqVt27bZ3zcMQ2PHjlXdunUVFBSkhIQEffvttw7XyMnJUVJSksLDw1WzZk3169dP+fn5LsVBggAAgBmjHA4X/Pzzz7rxxhtVo0YNffLJJ9q7d6+mTp2qWrVq2cdMmTJFM2bM0Jw5c7R582aFhIQoMTFRhYWF9jFJSUnas2ePUlJStHz5cq1fv14PP/ywS7HQYgAAwER53cWQl5fncD4gIEABAQFO41944QXFxsZq3rx59nONGzc+73qGpk+frqefflrdu3eXJC1cuFBRUVFaunSpevfurX379mnVqlXaunWr2rdvL0maOXOmunbtqhdffFExMTFlir1MCcLHH39cpotJ0t13313msQAAeIPY2FiH1+PGjdP48eOdxn388cdKTEzUPffco3Xr1qlevXp69NFHNWDAAEnSoUOHlJWVpYSEBPuciIgIdejQQampqerdu7dSU1NVs2ZNe3IgSQkJCfLx8dHmzZv1l7/8pUwxlylB6NGjR5kuZrFYZLVayzQWAIBqoRwWGmZmZio8PNz++kLVA0k6ePCgZs+ereHDh+uf//yntm7dqscff1z+/v5KTk5WVlaWJCkqKsphXlRUlP29rKws1alTx+F9Pz8/RUZG2seURZkSBJvNVuYLAgBwqSivFkN4eLhDgmDGZrOpffv2eu655yRJ11xzjXbv3q05c+YoOTn5ouO4GG4tUjx/QQQAAJecSl6kWLduXcXFxTmca9GihTIyMiRJ0dHRkqTs7GyHMdnZ2fb3oqOjdeLECYf3S0tLlZOTYx9TFi4nCFarVZMmTVK9evUUGhqqgwcPSpLGjBmjN99809XLAQCAX9x4441KT093OPfNN9+oYcOGks4uWIyOjtbq1avt7+fl5Wnz5s2Kj4+XJMXHxys3N1fbt2+3j1mzZo1sNps6dOhQ5lhcThCeffZZzZ8/X1OmTJG/v7/9fMuWLfXGG2+4ejkAAKowSzkcZTds2DBt2rRJzz33nA4cOKDFixfr9ddf16BBg85GY7Fo6NCheuaZZ/Txxx9r165d6tOnj2JiYuzrBVu0aKEuXbpowIAB2rJli7766isNHjxYvXv3LvMdDNJFJAgLFy7U66+/rqSkJPn6+trPt27dWvv373f1cgAAVF2V3GK49tpr9dFHH+ntt99Wy5YtNWnSJE2fPl1JSUn2MSNHjtRjjz2mhx9+WNdee63y8/O1atUqBQYG2scsWrRIzZs312233aauXbvqpptu0uuvv+5SLC7vg3D06FE1bdrU6bzNZlNJSYmrlwMAAOe58847deedd5q+b7FYNHHiRE2cONF0TGRkpBYvXuxWHC5XEOLi4vTll186nX///fd1zTXXuBUMAABVSiVXEKoSlysIY8eOVXJyso4ePSqbzaYPP/xQ6enpWrhwoZYvX14RMQIA4Ble/DRHlysI3bt317Jly/T5558rJCREY8eO1b59+7Rs2TLdfvvtFREjAACoZBf1LIabb75ZKSkp5R0LAABVijc/7vmiH9a0bds27du3T9LZdQnt2rUrt6AAAKgS3F1H4E0JwpEjR3Tffffpq6++Us2aNSVJubm5uuGGG/TOO++ofv365R0jAACoZC6vQejfv79KSkq0b98+5eTkKCcnR/v27ZPNZlP//v0rIkYAADzj3CJFd45qyuUKwrp167Rx40Y1a9bMfq5Zs2aaOXOmbr755nINDgAAT7IYZw935ldXLicIsbGxF9wQyWq1urSFIwAAVZ4Xr0FwucXwr3/9S4899pi2bdtmP7dt2zYNGTJEL774YrkGBwAAPKNMFYRatWrJYvm1j1JQUKAOHTrIz+/s9NLSUvn5+emhhx6yPywCAIBqz4s3SipTgjB9+vQKDgMAgCrIi1sMZUoQkpOTKzoOAABQhVz0RkmSVFhYqOLiYodz4eHhbgUEAECV4cUVBJcXKRYUFGjw4MGqU6eOQkJCVKtWLYcDAIBLhhc/zdHlBGHkyJFas2aNZs+erYCAAL3xxhuaMGGCYmJitHDhwoqIEQAAVDKXWwzLli3TwoULdeutt6pv3766+eab1bRpUzVs2FCLFi1SUlJSRcQJAEDl8+K7GFyuIOTk5KhJkyaSzq43yMnJkSTddNNNWr9+fflGBwCAB53bSdGdo7pyOUFo0qSJDh06JElq3ry5lixZIulsZeHcw5sAAED15nKC0LdvX3399deSpFGjRmnWrFkKDAzUsGHDNGLEiHIPEAAAj/HiRYour0EYNmyY/X8nJCRo//792r59u5o2baqrr766XIMDAACe4dY+CJLUsGFDNWzYsDxiAQCgSrHIzac5llskla9MCcKMGTPKfMHHH3/8ooMBAABVQ5kShGnTppXpYhaLpdokCH+5spX8LDU8HQZQIfLvaenpEIAKU1pSKH1USR/mxbc5lilBOHfXAgAAXoWtlgEAAH7l9iJFAAAuWV5cQSBBAADAhLu7IXrVTooAAODSRwUBAAAzXtxiuKgKwpdffqn7779f8fHxOnr0qCTp3//+tzZs2FCuwQEA4FFevNWyywnCBx98oMTERAUFBWnnzp0qKiqSJJ08eVLPPfdcuQcIAAAqn8sJwjPPPKM5c+Zo7ty5qlHj142GbrzxRu3YsaNcgwMAwJO8+XHPLq9BSE9P1y233OJ0PiIiQrm5ueUREwAAVYMX76TocgUhOjpaBw4ccDq/YcMGNWnSpFyCAgCgSmANQtkNGDBAQ4YM0ebNm2WxWHTs2DEtWrRITzzxhAYOHFgRMQIAgErmcoth1KhRstlsuu2223T69GndcsstCggI0BNPPKHHHnusImIEAMAjvHmjJJcTBIvFoqeeekojRozQgQMHlJ+fr7i4OIWGhlZEfAAAeI4X74Nw0Rsl+fv7Ky4urjxjAQAAVYTLCUKnTp1ksZivylyzZo1bAQEAUGW4e6uiN1UQ2rRp4/C6pKREaWlp2r17t5KTk8srLgAAPI8WQ9lNmzbtgufHjx+v/Px8twMCAACeV25Pc7z//vv11ltvldflAADwPC/eB6HcnuaYmpqqwMDA8rocAAAex22OLujZs6fDa8MwdPz4cW3btk1jxowpt8AAAIDnuJwgREREOLz28fFRs2bNNHHiRHXu3LncAgMAAJ7jUoJgtVrVt29ftWrVSrVq1aqomAAAqBq8+C4GlxYp+vr6qnPnzjy1EQDgFbz5cc8u38XQsmVLHTx4sCJiAQAAVYTLCcIzzzyjJ554QsuXL9fx48eVl5fncAAAcEnxwlscJRfWIEycOFH/+Mc/1LVrV0nS3Xff7bDlsmEYslgsslqt5R8lAACe4MVrEMqcIEyYMEGPPPKIvvjii4qMBwAAVAFlThAM42wa1LFjxwoLBgCAqoSNksro957iCADAJYcWQ9lceeWVf5gk5OTkuBUQAADwPJcShAkTJjjtpAgAwKWKFkMZ9e7dW3Xq1KmoWAAAqFq8uMVQ5n0QWH8AAID3cPkuBgAAvIYXVxDKnCDYbLaKjAMAgCqHNQgAAMCZF1cQXH4WAwAAuPRRQQAAwIwXVxBIEAAAMOHNaxBoMQAAACdUEAAAMEOLAQAA/BYtBgAAgPNQQQAAwAwtBgAA4MSLEwRaDAAAVFHPP/+8LBaLhg4daj9XWFioQYMGqXbt2goNDVWvXr2UnZ3tMC8jI0PdunVTcHCw6tSpoxEjRqi0tNSlzyZBAADAhKUcjou1detWvfbaa7r66qsdzg8bNkzLli3Te++9p3Xr1unYsWPq2bOn/X2r1apu3bqpuLhYGzdu1IIFCzR//nyNHTvWpc8nQQAAwIxRDoekvLw8h6OoqOh3PzY/P19JSUmaO3euatWqZT9/8uRJvfnmm3rppZf05z//We3atdO8efO0ceNGbdq0SZL02Wefae/evfrPf/6jNm3a6I477tCkSZM0a9YsFRcXl/mrkyAAAGDi3G2O7hySFBsbq4iICPsxefLk3/3cQYMGqVu3bkpISHA4v337dpWUlDicb968uRo0aKDU1FRJUmpqqlq1aqWoqCj7mMTEROXl5WnPnj1l/u4sUgQAoIJlZmYqPDzc/jogIMB07DvvvKMdO3Zo69atTu9lZWXJ399fNWvWdDgfFRWlrKws+5jzk4Nz7597r6xIEAAAMFNOdzGEh4c7JAhmMjMzNWTIEKWkpCgwMNCND3YfLQYAAH6Pm+sPXLF9+3adOHFCbdu2lZ+fn/z8/LRu3TrNmDFDfn5+ioqKUnFxsXJzcx3mZWdnKzo6WpIUHR3tdFfDudfnxpQFCQIAAFXEbbfdpl27diktLc1+tG/fXklJSfb/XaNGDa1evdo+Jz09XRkZGYqPj5ckxcfHa9euXTpx4oR9TEpKisLDwxUXF1fmWGgxAABgorKfxRAWFqaWLVs6nAsJCVHt2rXt5/v166fhw4crMjJS4eHheuyxxxQfH6/rr79ektS5c2fFxcXpgQce0JQpU5SVlaWnn35agwYN+t21D79FggAAgJkquJPitGnT5OPjo169eqmoqEiJiYl69dVX7e/7+vpq+fLlGjhwoOLj4xUSEqLk5GRNnDjRpc8hQQAAoApbu3atw+vAwEDNmjVLs2bNMp3TsGFDrVy50q3PJUEAAMCENz/umQQBAAAzVbDFUFm4iwEAADihggAAgAlaDAAAwJkXtxhIEAAAMOPFCQJrEAAAgBMqCAAAmGANAgAAcEaLAQAA4FdUEAAAMGExDFmMiy8DuDPX00gQAAAwQ4sBAADgV1QQAAAwwV0MAADAGS0GAACAX1FBAADABC0GAADgzItbDCQIAACY8OYKAmsQAACAEyoIAACYocUAAAAupDq3CdxBiwEAADihggAAgBnDOHu4M7+aIkEAAMAEdzEAAACchwoCAABmuIsBAAD8lsV29nBnfnVFiwEAADihgoByUzu6RP2eOqZrO51SQJBNxw4HaOqwWH37v2BJ0qfHvr7gvLmT6ur92XUqM1Tgd/W4ca/+ctNe1Y08JUk6dLyW5n3aVpv2NZAk3R2/T7e3O6BmsT8qJLBEiaOSlX8mwOEaL/Rfpab1f1Kt0EKdOu2vbd/U0+yPO+jHvJBK/z5wAy0GwD2hEaV66b/f6n8bQ/X0/U2U+5Ov6jUpVv5JX/uY3q3jHOZc++dTGjY1UxtWRFR2uMDv+iE3RHOWXafMHyJkkaE7rvtGz/f/TH3/1VOHsiIV6F+qzftjtXl/rAbeteWC19hxIEYLU67Rj3nB+lNEgQb32KxnHvpcj0zvXsnfBu7gLgYPWb9+ve666y7FxMTIYrFo6dKlfzhn7dq1atu2rQICAtS0aVPNnz+/wuPEH7t30An9eMxfU4c1UHpasLIzA7RjXZiOf//rX1U//1DD4YhPPKmvvwpVVkbA71wZqHxf7Wmo1L0NdOSHCGX+UFOvr7hOZ4pq6KpGJyRJS9a10n8+b6M9h80rX++uvVp7vo9S9s9h2n04Wv/5vLWuapgtX59q3JT2Ruf2QXDnqKY8miAUFBSodevWmjVrVpnGHzp0SN26dVOnTp2UlpamoUOHqn///vr0008rOFL8kes75+mbr4P01GuH9e7/9mjWZ+m6428/mY6veVmJrrstT5++E1mJUQKu87HYdNs1BxQYUKLdh6Iu6hphwYXq3O6Adh2OktXG0i9UDx5tMdxxxx264447yjx+zpw5aty4saZOnSpJatGihTZs2KBp06YpMTHxgnOKiopUVFRkf52Xl+de0Ligug2KdWefn/Th63/SOzPr6MrWZzRw0lGVlFj0+XvOScDt9/6sM/m+2rCS9gKqpiZ1c/TasKXy97PqTFEN/fPNzjqcXculawy8a7N63bxHQQGl2n2ojka83qWCokVFocVQTaSmpiohIcHhXGJiolJTU03nTJ48WREREfYjNja2osP0ShYf6cDuIM17vq6+2x2sTxbV1ieLa6vbAxeuIiT2ztGaj2qqpKha/QjCi2SciNCDU3rp4Zd6aOlXcXoqaa0aRf3s0jUWr2mtvv/qqaGvdpXVsGjM/V+oWq9a80ZGORzVVLX6r3NWVpaiohxLfFFRUcrLy9OZM2cuOGf06NE6efKk/cjMzKyMUL1Ozgk/ff9NoMO5zG8DVKdesdPYltflK7ZpkVYtrl1Z4QEuK7X66uiPEUo/8ifNWX6dDhytrXs67nLpGicLApX5Q01tTa+vcfNv0w1XZdrXMQBV3SV/F0NAQIACAlgEV9H2bg1R7OVFDufqNSnSiaP+TmMT78vRN18H6eDeoMoKD3Cbj8WQv9/FLzD0+eXPMX8/azlFhMpAi6GaiI6OVnZ2tsO57OxshYeHKyiIXzae9OHrf1LztgXq/Vi2YhoVqdNfflbX+3P08bzLHMYFh1p1y10ntWoxixNRdT1y5xa1vvy4oiNPqUndHD1y5xZd0/SYPtveVJIUGXZaV9T7UfUvO7um6fK6Obqi3o8KCy6UJMU1PKFeN+/WFfV+VFStU2p7xVGN77NaR34Iv+iFjvAQL76LoVpVEOLj47Vy5UqHcykpKYqPj/dQRDjnm6+DNbFfY/UdfVxJw7KVlemvOWNj9MVHjou6OnbPlSyGvljq2mIvoDLVDDujMUlfqHbEaRWc8deBY7U1fE5XbU2vL+nsRkr97thhH//qkGWSpGcXddTKLc1UWOynjlcfVr87tivQv1Q/5QVr8776GvNZW5VYfS/4mUBV49EEIT8/XwcOHLC/PnTokNLS0hQZGakGDRpo9OjROnr0qBYuXChJeuSRR/TKK69o5MiReuihh7RmzRotWbJEK1as8NRXwHk2fx6uzZ+H/+6YTxbV1ieLWHuAqu35tzv+7vtvrWqvt1a1N33/4PFIPT7rzvIOCx7gzS0GjyYI27ZtU6dOneyvhw8fLklKTk7W/Pnzdfz4cWVkZNjfb9y4sVasWKFhw4bp5ZdfVv369fXGG2+Y3uIIAIBb2GrZM2699VYZv9OfudAuibfeeqt27txZgVEBAIBqtQYBAIDKRIsBAAA4sxlnD3fmV1MkCAAAmPHiNQjVah8EAABQOaggAABgwiI31yCUWySVjwQBAAAz7u6GWI13UqTFAAAAnFBBAADABLc5AgAAZ9zFAAAA8CsqCAAAmLAYhixuLDR0Z66nkSAAAGDG9svhzvxqihYDAABwQgUBAAATtBgAAIAzL76LgQQBAAAz7KQIAADwKyoIAACYYCdFAADgjBYDAADAr6ggAABgwmI7e7gzv7oiQQAAwAwtBgAAgF9RQQAAwAwbJQEAgN/y5q2WaTEAAAAnVBAAADDDIkUAAODEkGRz43AxP5g8ebKuvfZahYWFqU6dOurRo4fS09MdxhQWFmrQoEGqXbu2QkND1atXL2VnZzuMycjIULdu3RQcHKw6depoxIgRKi0tdSkWEgQAAEycW4PgzuGKdevWadCgQdq0aZNSUlJUUlKizp07q6CgwD5m2LBhWrZsmd577z2tW7dOx44dU8+ePe3vW61WdevWTcXFxdq4caMWLFig+fPna+zYsS7FQosBAIAqYtWqVQ6v58+frzp16mj79u265ZZbdPLkSb355ptavHix/vznP0uS5s2bpxYtWmjTpk26/vrr9dlnn2nv3r36/PPPFRUVpTZt2mjSpEl68sknNX78ePn7+5cpFioIAACYMfTrOoSLOs5eJi8vz+EoKioq08efPHlSkhQZGSlJ2r59u0pKSpSQkGAf07x5czVo0ECpqamSpNTUVLVq1UpRUVH2MYmJicrLy9OePXvK/NVJEAAAMONWcvDrAsfY2FhFRETYj8mTJ//hR9tsNg0dOlQ33nijWrZsKUnKysqSv7+/atas6TA2KipKWVlZ9jHnJwfn3j/3XlnRYgAAoIJlZmYqPDzc/jogIOAP5wwaNEi7d+/Whg0bKjI0UyQIAACYsUmyuDlfUnh4uEOC8EcGDx6s5cuXa/369apfv779fHR0tIqLi5Wbm+tQRcjOzlZ0dLR9zJYtWxyud+4uh3NjyoIWAwAAJir7LgbDMDR48GB99NFHWrNmjRo3buzwfrt27VSjRg2tXr3afi49PV0ZGRmKj4+XJMXHx2vXrl06ceKEfUxKSorCw8MVFxdX5lioIAAAUEUMGjRIixcv1n//+1+FhYXZ1wxEREQoKChIERER6tevn4YPH67IyEiFh4frscceU3x8vK6//npJUufOnRUXF6cHHnhAU6ZMUVZWlp5++mkNGjSoTK2Nc0gQAAAwU8k7Kc6ePVuSdOuttzqcnzdvnh588EFJ0rRp0+Tj46NevXqpqKhIiYmJevXVV+1jfX19tXz5cg0cOFDx8fEKCQlRcnKyJk6c6FIsJAgAAJip5ATBKMP4wMBAzZo1S7NmzTId07BhQ61cudKlz/4t1iAAAAAnVBAAADDjxQ9rIkEAAMBMOd3mWB2RIAAAYOJiblX87fzqijUIAADACRUEAADMsAYBAAA4sRmSxY1f8rbqmyDQYgAAAE6oIAAAYIYWAwAAcOZmgqDqmyDQYgAAAE6oIAAAYIYWAwAAcGIz5FabgLsYAADApYQKAgAAZgzb2cOd+dUUCQIAAGZYgwAAAJywBgEAAOBXVBAAADBDiwEAADgx5GaCUG6RVDpaDAAAwAkVBAAAzNBiAAAATmw2SW7sZWCrvvsg0GIAAABOqCAAAGCGFgMAAHDixQkCLQYAAOCECgIAAGa8eKtlEgQAAEwYhk2GG09kdGeup5EgAABgxjDcqwKwBgEAAFxKqCAAAGDGcHMNQjWuIJAgAABgxmaTLG6sI6jGaxBoMQAAACdUEAAAMEOLAQAA/JZhs8lwo8VQnW9zpMUAAACcUEEAAMAMLQYAAODEZkgW70wQaDEAAAAnVBAAADBjGJLc2Qeh+lYQSBAAADBh2AwZbrQYDBIEAAAuQYZN7lUQuM0RAABcQqggAABgghYDAABw5sUtBq9LEM5lc6UqcWvvC6AqKy0p9HQIQIWx/vLzXRl/nbv7u6JUJeUXTCWzGNW5/nERjhw5otjYWE+HAQBwU2ZmpurXr18h1y4sLFTjxo2VlZXl9rWio6N16NAhBQYGlkNklcfrEgSbzaZjx44pLCxMFovF0+F4hby8PMXGxiozM1Ph4eGeDgcoV/x8Vz7DMHTq1CnFxMTIx6fi1toXFhaquLjY7ev4+/tXu+RA8sIWg4+PT4VlnPh94eHh/AcUlyx+vitXREREhX9GYGBgtfzFXl64zREAADghQQAAAE5IEFDhAgICNG7cOAUEBHg6FKDc8fONS5XXLVIEAAB/jAoCAABwQoIAAACckCAAAAAnJAgAAMAJCQLKxaxZs9SoUSMFBgaqQ4cO2rJly++Of++999S8eXMFBgaqVatWWrlyZSVFCrhm/fr1uuuuuxQTEyOLxaKlS5f+4Zy1a9eqbdu2CggIUNOmTTV//vwKjxMobyQIcNu7776r4cOHa9y4cdqxY4dat26txMREnThx4oLjN27cqPvuu0/9+vXTzp071aNHD/Xo0UO7d++u5MiBP1ZQUKDWrVtr1qxZZRp/6NAhdevWTZ06dVJaWpqGDh2q/v3769NPP63gSIHyxW2OcFuHDh107bXX6pVXXpF09nkXsbGxeuyxxzRq1Cin8X/9619VUFCg5cuX289df/31atOmjebMmVNpcQOuslgs+uijj9SjRw/TMU8++aRWrFjhkPD27t1bubm5WrVqVSVECZQPKghwS3FxsbZv366EhAT7OR8fHyUkJCg1NfWCc1JTUx3GS1JiYqLpeKA64ecblwoSBLjlxx9/lNVqVVRUlMP5qKgo08ekZmVluTQeqE7Mfr7z8vJ05swZD0UFuI4EAQAAOCFBgFsuu+wy+fr6Kjs72+F8dna2oqOjLzgnOjrapfFAdWL28x0eHq6goCAPRQW4jgQBbvH391e7du20evVq+zmbzabVq1crPj7+gnPi4+MdxktSSkqK6XigOuHnG5cKEgS4bfjw4Zo7d64WLFigffv2aeDAgSooKFDfvn0lSX369NHo0aPt44cMGaJVq1Zp6tSp2r9/v8aPH69t27Zp8ODBnvoKgKn8/HylpaUpLS1N0tnbGNPS0pSRkSFJGj16tPr06WMf/8gjj+jgwYMaOXKk9u/fr1dffVVLlizRsGHDPBE+cPEMoBzMnDnTaNCggeHv729cd911xqZNm+zvdezY0UhOTnYYv2TJEuPKK680/P39jauuuspYsWJFJUcMlM0XX3xhSHI6zv1MJycnGx07dnSa06ZNG8Pf399o0qSJMW/evEqPG3AX+yAAAAAntBgAAIATEgQAAOCEBAEAADghQQAAAE5IEAAAgBMSBAAA4IQEAQAAOCFBAAAATkgQAA948MEH1aNHD/vrW2+9VUOHDq30ONauXSuLxaLc3FzTMRaLRUuXLi3zNcePH682bdq4Fdfhw4dlsVjs2xsDqHwkCMAvHnzwQVksFlksFvn7+6tp06aaOHGiSktLK/yzP/zwQ02aNKlMY8vySx0A3OXn6QCAqqRLly6aN2+eioqKtHLlSg0aNEg1atRweNjUOcXFxfL39y+Xz42MjCyX6wBAeaGCAJwnICBA0dHRatiwoQYOHKiEhAR9/PHHkn5tCzz77LOKiYlRs2bNJEmZmZm69957VbNmTUVGRqp79+46fPiw/ZpWq1XDhw9XzZo1Vbt2bY0cOVK/fQTKb1sMRUVFevLJJxUbG6uAgAA1bdpUb775pg4fPqxOnTpJkmrVqiWLxaIHH3xQ0tnHbE+ePFmNGzdWUFCQWrdurffff9/hc1auXKkrr7xSQUFB6tSpk0OcZfXkk0/qyiuvVHBwsJo0aaIxY8aopKTEadxrr72m2NhYBQcH695779XJkycd3n/jjTfUokULBQYGqnnz5nr11VddjgVAxSFBAH5HUFCQiouL7a9Xr16t9PR0paSkaPny5SopKVFiYqLCwsL05Zdf6quvvlJoaKi6dOlinzd16lTNnz9fb731ljZs2KCcnBx99NFHv/u5ffr00dtvv60ZM2Zo3759eu211xQaGqrY2Fh98MEHkqT09HQdP35cL7/8siRp8uTJWrhwoebMmaM9e/Zo2LBhuv/++7Vu3TpJZxOZnj176q677lJaWpr69++vUaNGufzPJCwsTPPnz9fevXv18ssva+7cuZo2bZrDmAMHDmjJkiVatmyZVq1apZ07d+rRRx+1v79o0SKNHTtWzz77rPbt26fnnntOY8aM0YIFC1yOB0AF8fDTJIEqIzk52ejevbthGIZhs9mMlJQUIyAgwHjiiSfs70dFRRlFRUX2Of/+97+NZs2aGTabzX6uqKjICAoKMj799FPDMAyjbt26xpQpU+zvl5SUGPXr17d/lmGcfST2kCFDDMMwjPT0dEOSkZKScsE4zz1++Oeff7afKywsNIKDg42NGzc6jO3Xr59x3333GYZhGKNHjzbi4uIc3n/yySedrvVbkoyPPvrI9P1//etfRrt27eyvx40bZ/j6+hpHjhyxn/vkk08MHx8f4/jx44ZhGMbll19uLF682OE6kyZNMuLj4w3DMIxDhw4ZkoydO3eafi6AisUaBOA8y5cvV2hoqEpKSmSz2fS3v/1N48ePt7/fqlUrh3UHX3/9tQ4cOKCwsDCH6xQWFuq7777TyZMndfz4cXXo0MH+np+fn9q3b+/UZjgnLS1Nvr6+6tixY5njPnDggE6fPq3bb7/d4XxxcbGuueYaSdK+ffsc4pCk+Pj4Mn/GOe+++65mzJih7777Tvn5+SotLVV4eLjDmAYNGqhevXoOn2Oz2ZSenq6wsDB999136tevnwYMGGAfU1paqoiICJfjAVAxSBCA83Tq1EmzZ8+Wv7+/YmJi5Ofn+K9ISEiIw+v8/Hy1a9dOixYtcrrWn/70p4uKISgoyOU5+fn5kqQVK1Y4/GKWzq6rKC+pqalKSkrShAkTlJiYqIiICL3zzjuaOnWqy7HOnTvXKWHx9fUtt1gBuIcEAThPSEiImjZtWubxbdu21bvvvqs6deo4/RV9Tt26dbV582bdcsstks7+pbx9+3a1bdv2guNbtWolm82mdevWKSEhwen9cxUMq9VqPxcXF6eAgABlZGSYVh5atGhhX3B5zqZNm/74S55n48aNatiwoZ566in7ue+//95pXEZGho4dO6aYmBj75/j4+KhZs2aKiopSTEyMDh48qKSkJJc+H0DlYZEi4IakpCRddtll6t69u7788ksdOnRIa9eu1eOPP64jR45IkoYMGaLnn39eS5cu1f79+/Xoo4/+7h4GjRo1UnJysh566CEtXbrUfs0lS5ZIkho2bCiLxaLly5frhx9+UH5+vsLCwvTEE09o2LBhWrBggb777jvt2LFDM2fOtC/8e+SRR/Ttt99qxIgRSk9P1+LFizV//nyXvu8VV1yhjIwMvfPOO/ruu+80Y8aMCy64DAwMVHJysr7++mt9+eWXevzxx3XvvfcqOjpakjRhwgRNnjxZM2bM0DfffKNdu3Zp3rx5eumll1yKB0DFIUEA3BAcHKz169erQYMG6tmzp1q0aKF+/fqpsLDQXlH4xz/+oQceeEDJycmKj49XWFiY/vKXv/zudWfPnq3/+7//06OPPqrmzZtrwIABKigokCTVq1dPEyZM0KhRoxQVFaXBgwdLkiZNmqQxY8Zo8uTJatGihbp06aIVK1aocePGks6uC/jggw+0dOlStW7dWnPmzNFzzz3n0ve9++67NWzYMA0ePFht2rTRxo0bNWbMGKdxTZs2Vc+ePdW1a1d17txZV199tcNtjP3799cbb7yhefPmqVWrVurYsaPmz59vjxWA51kMs5VSAADAa1FBAAAATkgQAACAExIEAADghAQBAAA4IUEAAABOSBAAAIATEgQAAOCEBAEAADghQQAAAE5IEAAAgBMSBAAA4OT/AWoJZReLCJHeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmd = ConfusionMatrixDisplay(cnf)\n",
    "cmd=cmd.from_predictions(test_labels, yhat)\n",
    "cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94116b",
   "metadata": {},
   "source": [
    "13 false negatives and 188 false positives out of 1460 predictions. False negatives represent pneumonia x-rays classified as normal and are more harmful than false positives. 1% false negative rate is acceptably low.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a286cb",
   "metadata": {},
   "source": [
    "# Results/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed37f0",
   "metadata": {},
   "source": [
    "Model1a performed best.  The chosen model's accuracy on the holdout test set was 86%.  This can be compared to guessing based on the sample balance, which would yield 73% accuracy.  The false negative rate was very low at 1%. Thus this model can be used as a check by the radiologist, for instance on x-rays that they are less certain about.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9f9ad",
   "metadata": {},
   "source": [
    "# Future work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a773464",
   "metadata": {},
   "source": [
    "- Use CNN\n",
    "- Alter activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5c336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dash-env)",
   "language": "python",
   "name": "dash-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
