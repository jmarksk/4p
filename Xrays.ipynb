{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47dd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49c226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945e9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=Path('./data/archive/chest_xray/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b29a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c68a5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = i\n",
    "#np=i +''\n",
    "#pp=i +''\n",
    "#f=[x for x in i.iterdir()]\n",
    "#batch_size =len(f)\n",
    "# Get all the data in the directory data/train, and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=5216, color_mode = \"grayscale\")\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f539ed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216, 64, 64, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c26c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "val_data_dir = './data/archive/chest_xray/val'\n",
    "test_data_dir = './data/archive/chest_xray/test'\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "        val_data_dir, \n",
    "        target_size=(64, 64), batch_size= 16, color_mode = \"grayscale\")\n",
    "\n",
    "# Get all the data in the directory data/validation (132 images), and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=234+390, color_mode = \"grayscale\")\n",
    "\n",
    "\n",
    "# Create the datasets\n",
    "val_images, val_labels = next(val_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8156512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOS0uMLYz5i3tJGFB+X5TuU55BPQEcEHnrjIOvYafvhMiIcqecV0MVqrBZGykjnBXGMn1q8kP8adAACT1Xio3Z3RgJiuc+5xXPalA90x2lljVQoGew9aw7yyP9mxzBB8nyMR39D/AC/OuUvI/KkMqdV++P7wrrLSZbXT4yRln5xnr/kV1ukgtaELwzGnX880V5DGqqsUWfvHG4561vQJ+6Vm5crwPXFYF/qcllfRKiLIBy6+1WriKI2kjKu1XUMDnkA1z6TQmF7ObGGB4Pc/400+G9Gmto7GPzvtk4TNzIo8oSMjmNAwyVJZgDxg+X0zXLXF1vn3dB0X869D8P8AzRK3YKTz6jpTbvyrj5t5LDqfXtketdFbf8eg2n5dmN1cPqcW3U2cMynaPmPQda3UDvoyPJjccrx3HBz+tcNqF0sV7sflS2D9DWpY6lILieISFJ4kBifAIcrgrkHg8gH8Ae3PFXkuJ4FHOZVHX3r1LRwF0SR2Yr+6wCOoyR/n8an00G5iQmMLs4wR1H+c108ES/Z84wSPu54rk/EVvIu2VY8rnawVecVLpi+boUmGDBSCvqOuf6fnXl+uXOzXWjBAGT+Y4H9atWcr7cJIFkAVRlQeAR0/L8vxrAaQz38TD7vmKF/MV7BAGi8NLtUsxwVHqQD/AI0/QFJT5pwZGySRk9K7KL/Uj5ecYBrF1i0We2kUvh+o9j25rJ0fm2uo9uCF3HHQ8jB/nXkviJV/4SO8UdVIB/EZ/rUdnfGJ1Epxjo4/rUNpCZtQtowcHzAfy5/pXr1+I49HtLUuEIXLgck57fkBUOjzJDeQrsfbu28t1zXe26nyPvcYznHSs+ZRcBkVS3bg1jWNnJZX0kTkmOcN5Rx69uPwryLxJA0Pia834+dgw+mAP6VQCA1qeFLb7V4mt0aPcq8k+nI/pmvRVtJtU1GaY52k/LnkD0H4VvWmmLaspABYc8qOK3olQRgZP41nqJVmkUxhgwIx359K5/z73Sbxr5rOZbZiSySKyrjHYk4z7+nSuE+I2n/ZPEUV2AfLu0JQkY6YPI6g/OPTjB7muYTnFdJ8PYi2uSTcFVXp7gf/AF69ds7eOCMYX8KtsCcY49hVrgAHb29KhikkhLhWIVwQfcVS1W2e6gIb94QCBu5K/TP0HHT2rzD4kvKV0oSqyFVIVCwIVTyAAAAD68c8VxadBX//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAALo0lEQVR4AUVXy6tl2Vlf61vP/Trn3HPf91ZudSWd6hCaTmihA4qgBAQRnOhAMspMwbH4D2SmTgI6yiCDDBwIEgIZOBOURCLRNpq2q7rrXff9OO+993r62zdC7oW6dfZZ+1vf+n6P71v8I0nWMkYps4pLNf3iJ8YosonGfkOiGckl0zw49/y4eurY7HrxgJ+rEFxMz1VTLyxRZop+JEgqpa2Rffee5Jn12bS8sdac3bjlfNNtzqS9vLw5XRJN0j6TWMMfdevWSAq8EOIPJLHx7khJIe/8iDMjrNCrXOrA1kEkt1kIPl5paYRS+avbnAuVBH/YXUaJ1K3mPxRaE2eNLsjVdl/LEZkH233roxqnjnXrHN99tSbkKPTrTRCMU8U57bB1zz+sxkawwK1SDameG3ESbewcjdLKrGvLzwop2oLSC5eSp7heHuXNTZ9Cl2O8cDVxo3NZCWLJy7EoOBM3li9jozPX9ViFTmllJoG/5SiS0Zabhr9T4BCZGB2mJQkehVZVqURBTcMKSSU7jVxEH21pFXOc98vsEJWkLAWAevMbGxYpK89T3k1SMqolH1CUQhuTjXTChPVY884UXciZB97fbV9G7bmmFJIqTuUH/7HkOSWWgSKJYBDAaEIFq5q79Vhwnze8KdgmiaKyJCqhS+BsVTmJQhr66qLxnlHupNsjEUMbU0qE+L5bMMaq8nalUkK+V5FXpJUIxA+lqmhke8GsWb9SR3uM95u/iiZTSMcmC5E5isQZAtfy41mc02rBy3KsHUh3d3d+J/d3ip0GuwsvFDs2ZWH+nI/3fkdQ5kmEhGWSdMyoQ3m96RdxCThEoUTEV7G/+uWq2bBMEiizFO6W/cP3J1//Xrv4QyLwIXJ6O4uiJxU42eWbnvWLK0ZeEc8xyBhO+/VPNhMW+g4p8PWt9RQWf6lG5k+ElXk3aPmMOWTSlZwm9ZMu6AmwBMAip6D74rQn8j//UPkUUSnnu8vd7WdKfMec/z7/a4lqhYC0AG1SUYEd22UoFrqWNvS5rDfiJhZtooZNLlVMfkBvNi8irR+02z/UH5HQ3fitZ6AWeZDVsulW275d3ckWZ9Okcrc+LffH1LWRu9x5xYJL7lNDP9pZfee9f7D0OOv/ZjxhZ86zUMLLCzJUunXkqqhkmF+tuouLqqnmCJjhGwkclv0nVTM7/pu/iAsK5FLIKbrIQU2eZk6VUWxSVqkLki0D90KwZ3NRbERmgRjSZNSoJ9osTv72z4hSfcFAJEc+50C2bJGG27Z2wxJK7mNisoulDafzgS0ZRhIju1U6v6x9uv3pd8k+x+s5ZkL9PJu+YZHbLyzHemRyQHmFbNTejnMsvOkLKAjS8q4LFzsm/ou5/FO3ItUhAYbfmJVkFXAoDleVsYbL9crRtuVNHtX7nsubHQHvZCa0Xp2x8m59pX/8rZKeQocZFHaRMS7WleDnT8ZsljNLZUkInhf+1Vxv1wW8MzAWRWRtl64mLT9fhJ9wVD/n7PoYc+CMZS5e6GtnRYdFSguap2jTlK6FntQ9vg0cvpS8a2Gr+rNR9yVAAzNwWYlswH7un3Npr5jXC+QgGGmYRMUO9auZqR0+Y10Mvtezp8fQ98f9nDhAiqs19ywyMPY266oYL7bslDwsCMxSNZUtr9LLvMRnCrnrgIx67gdlXqMoKYbMZsCWCc5X52li9opqZ3fLsHbTqbqqtj7QUwUdzcF2ysRd0B1EfCk79q1PKwRIwLEGl3nID6+jtu2mjwtT6lozroUQ62X7Qk2ZX6QR9AXfSMIthbigGH6Qf04ofUD1ybEstb870M9WJpcxdlzv2pKBIVwt2bU5iN1ZwWUWvmc/FWkueV9EodCRkuO5n2dCLUT75dFZXImiYcy1FKQUAXHrXTO+XhxM5eDAHFYqNArP1/XIkqUSnZBdwZkAMxB9JebyLouRCyZHjrc5mm/T3ma3NMEquLoP3NwBcc7Ot4Sc0Cp4tBANPjAw8Tkb89aWTVlKAvfvVa7k/iS4fmm2b/ayYI71oh/Il8RTLTYE8qSZoCQ5pBqUaI/2Q7/gopC53YSgjCFi73zlYUpP+TwqeFS+E/JVRuzMXzeGYFO0BhsyDnzyBJ22OoX/AC1js4yWJJput7lsthy7KlqUPAfsVy4DzCF6mB3+XKCxRFggMpAifr6IapNanmWpSoXZIlJVxhf7u+AZ1sOCFSe+WoH2nC9gEEgKBegzzqx6tXyZ90yQy8SRK6aHDGNJovmCe7130L60lKNi2E+oxR18gdE15XSGB4OZibw+0be1a/2SlwFoSawAaxscc9rYz+0BTTipEUYLoKmWVxBg4pI7Qq9gwEcUc/WLLpjRfrCwe3Q7OBzyFg2TS0X583cXEXAXG7gnx1n8ywecIckLgQcM44Bkel3Hwz2MBtpwSMxBNqCypWSqR5DxaoqOymHXqD86ktSvV7GV5xKfh66VZMtuzKPcXoTQuEoYu5GJ6z72ucuzYqucXU2hGsYvtxCGMYkizm6OCHsMdgxy8iM3qtRpJ33wBHVTWRaAJudWRL48p7JyaQ90CTgBiIhCoJNeEQ7AEnogQscbuvskeRbUPMKkw/Aw+CQwOqjUvJk2M2zKMAiAhhAhyiCFwFlABWwHEIS9XNbZzPrZIkHcAWrmHniiyFYV7BnbDWgqSWH9kAFy5gI8QwV49nCud6RYtlyWTVKpMtwHFqLDLtaoKE0V/Zl4A5fM6T7/4X3sDxjRKxg6TpHz6iY0qVIKuCK2S71AIkk6DDrkpYb0dxMSAjtQ9CELKDJL/IPPXwMzzHN5GK0/zSX6wzFBCxyDQARnc+wVffhsc2FRfw6zuK8ifAFUHCKhMx5I1Sx1bRZnpyCP5KuWSo2xDrWC4/XMdU5Oj+2CDiT/ABlz/EI9vyrEEEIKNZ27/hWaszLxjs3QAX0Uc2AYoZr4pYPp3qOyvTx3VjdoJHjn/38gJqTovRl1q9PXQ6qMmx1nY4go9xHrgkfTeHBj+mX1FbllRoUs8ToKf587JDAEy91IHtafLdF6ZD9CNtu6gtvrfO4YV9mr28nz1Xp+/FtPRzTSXVC/3h7HRULpfXei528CM4Wdlpjft4ui5eAwVAoxKXbgc3m5/p///cbRa3H8S1dgHrkvBMwQLAIWU3UkftbzAgMuXbqijOiMhNEaRJFxvfFudnBwMqrE5Hy3nP+2AxPwc2+sbvDEDK5pZWMhUvb1w/1dzL6tlUXEdIrxy4fVs5Gbv53z/ge/CFP6+CNQ574OAxgIgLdYUX7aHTE+3j+u66nS3LJRGWxIvcR4HYO9YLPR9qSqih+vv6m+Bh8ZBDTUEqIcashNuqr95mFDhSlhYZxt3YhtpAdbKIJvaL18Tbal0T9X8e43/x5uNYwaQwL3ymZjccQrc/vutKmbcmjXQuEisYH4/WqklIy1+uDBZuVY87vsXzcn3+4GJmAwGVKQJe5DXy5tYpePMdOjC2DaUVwYMtEPCXa7V8ZvDt2j36OV+9nnm8cffv+PZiO8OZgK47XEDJ2gH9W/Y4qKYY4SEtFYMs52AEv6uZJfLMXLxnbLf3ysD/+r+uMXX7+BgAdfYHmFGvDIVKHGprRAFQJH2UKSXuEuwCmotbBn74X16qJ795vX6uRku/27rWvI8T4D5HjfF4TJY8Au77s1+gRUX4ow6FY7Fm/pVR+ljp/dnDT/1H46ev8/h/PfZ3AfILesZKsSt04jCC9llSMT65op4O1CNlHMzNYGt5PHn7hvPHryoP73+4ET4XGSwQ8eCYcnGKgIPjh4NvSO2+8gdVSDgvUiriSPxWprc/Nvj7q3AweGEg4rUafQ0DZAwf6QIJ4PnR5XOPyncxFfSje4X2IYqoy141Fs4Yq/YjOCDO2rohpn0hgqcUUBeZDOPUQdQTQAweMpNsPUl3bC9nvrW1g98hzWYqYg3IoKHKbA/RB/htw4+gnPuB3hMqUCqzD/MBotIpdxdqAJUyxa+3AARPX/B8r5Vs96vJm8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(train_images[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4d0728b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m(\u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "sum(train_labels[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629464a",
   "metadata": {},
   "source": [
    "Training class distribution is 1341 to 3875, about 1:3, normal to pneumonia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94911bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e57b7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_images.pickle', 'wb') as f:\n",
    "    pickle.dump(train_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc338a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f6a5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2b45cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_images.pickle', 'wb') as f:\n",
    "    pickle.dump(test_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "157ea59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e714122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_images.pickle', 'wb') as f:\n",
    "    pickle.dump(val_images, f)\n",
    "\n",
    "    \n",
    "with open('val_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(val_labels, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ab18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_images.pickle', 'rb') as g:\n",
    "    train_images2= pickle.load(g)\n",
    "with open('test_images.pickle', 'rb') as f:\n",
    "    test_images1= pickle.load(f)\n",
    "with open('val_images.pickle', 'rb') as f:\n",
    "    val_images1= pickle.load(f)\n",
    "    \n",
    "with open('train_labels.pickle', 'rb') as f:\n",
    "    train_labels1= pickle.load(f)\n",
    "with open('test_labels.pickle', 'rb') as f:\n",
    "    test_labels1= pickle.load(f)\n",
    "with open('val_labels.pickle', 'rb') as f:\n",
    "    val_labels1= pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfae61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.append(train_images2, val_images1, axis=0)\n",
    "t1 = np.append(train_labels1, val_labels1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19e7d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 64, 64, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.append(t, test_images1, axis=0)\n",
    "t1 = np.append(t1, test_labels1, axis=0)\n",
    "np.shape(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3916ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979ef6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.shape(t1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d21b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(t, t1) \n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ce2685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = np.reshape(test_images, (1464,64*64))\n",
    "np.shape(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "188f4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.reshape(train_images, (np.shape(train_images)[0],64*64))\n",
    "np.shape(train_images)\n",
    "\n",
    "val_images = np.reshape(val_images, (np.shape(val_images)[0],64*64))\n",
    "#np.shape(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f93439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#s = pd.Series(val)\n",
    "#s.sort_values(desc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "780d7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8f7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3294,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de40b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= train_labels[:,0]\n",
    "val_labels= val_labels[:,0]\n",
    "test_labels= test_labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d70db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f345afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 1s 7ms/step - loss: 0.2030 - acc: 0.7237 - val_loss: 0.1974 - val_acc: 0.7341\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1997 - acc: 0.7268 - val_loss: 0.1953 - val_acc: 0.7341\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1987 - acc: 0.7268 - val_loss: 0.1948 - val_acc: 0.7341\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1985 - acc: 0.7268 - val_loss: 0.1953 - val_acc: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2088bd940a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_labels, batch_size=64, epochs=4, validation_data =(val_images,val_labels))\n",
    "#model.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b635f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262273 (1.00 MB)\n",
      "Trainable params: 262273 (1.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e8f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "672e58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(64, input_shape = (4096,), activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(32, activation = 'tanh'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "#model.add(tf.keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39151cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'sgd', loss =  'mse', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f4a5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 0.2016 - acc: 0.7213 - val_loss: 0.1940 - val_acc: 0.7341\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1987 - acc: 0.7268 - val_loss: 0.1953 - val_acc: 0.7341\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1985 - acc: 0.7268 - val_loss: 0.1950 - val_acc: 0.7341\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1979 - acc: 0.7268 - val_loss: 0.1956 - val_acc: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2088dd344f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images,train_labels, batch_size=64, epochs=4, validation_data =(val_images,val_labels))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b1e9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264321 (1.01 MB)\n",
      "Trainable params: 264321 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cabd5276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.0728404e-03,  1.4095105e-02, -2.1492694e-02, ...,\n",
       "         -2.8256787e-02,  2.8004274e-02,  3.1000011e-02],\n",
       "        [-2.5070019e-02,  1.3704920e-02,  3.2803208e-02, ...,\n",
       "         -4.7790250e-03,  2.8032262e-03,  2.7362950e-02],\n",
       "        [ 3.0589547e-02,  6.6645285e-03,  8.7537095e-03, ...,\n",
       "          3.7712954e-02, -1.3464628e-02,  1.8712878e-03],\n",
       "        ...,\n",
       "        [ 1.3985578e-02,  9.1286628e-03,  1.6147584e-02, ...,\n",
       "         -2.5024969e-02,  1.8183509e-02,  5.2236021e-05],\n",
       "        [-5.3115725e-04, -3.5559043e-02, -3.4170367e-02, ...,\n",
       "         -3.2656953e-02,  2.5730340e-02, -2.4836130e-02],\n",
       "        [-3.2884590e-02, -3.4457389e-02,  1.5547834e-02, ...,\n",
       "         -1.1562874e-02,  8.0244187e-03, -1.2289748e-02]], dtype=float32),\n",
       " array([-2.9240821e-06, -2.7383935e-06,  0.0000000e+00, -4.5228194e-06,\n",
       "        -8.7135759e-06, -3.3489414e-06,  6.1052770e-06,  0.0000000e+00,\n",
       "         0.0000000e+00,  2.3563722e-07,  3.7414188e-06,  0.0000000e+00,\n",
       "         0.0000000e+00,  2.6570581e-06,  0.0000000e+00,  5.5980968e-06,\n",
       "        -1.8353072e-07,  3.9566003e-06, -4.1576212e-07, -1.9336890e-06,\n",
       "         2.1077565e-06, -1.0724152e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "        -4.0403393e-07, -3.7282177e-06,  5.2856740e-06, -6.0449056e-06,\n",
       "         0.0000000e+00,  0.0000000e+00,  3.9949250e-06,  0.0000000e+00,\n",
       "        -2.6473965e-06,  5.3699187e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "         7.8565690e-06,  1.9345605e-06,  0.0000000e+00, -3.2894195e-06,\n",
       "         2.4604444e-06,  0.0000000e+00,  0.0000000e+00,  6.2170488e-06,\n",
       "         3.0096026e-06,  0.0000000e+00, -1.0338060e-06, -1.3210541e-06,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -5.3523358e-06,\n",
       "        -2.3292089e-06, -5.3643585e-06,  0.0000000e+00,  5.3977728e-06,\n",
       "         0.0000000e+00,  6.2411746e-06,  2.9228727e-06, -6.8358045e-06,\n",
       "        -3.8304350e-07, -2.0494717e-07,  1.3376055e-07,  0.0000000e+00],\n",
       "       dtype=float32),\n",
       " array([[-0.12870692, -0.16498521,  0.12975906, ..., -0.15575364,\n",
       "          0.03371928, -0.13361539],\n",
       "        [ 0.01034237, -0.24932814,  0.2176447 , ...,  0.13317916,\n",
       "          0.20215772,  0.08643372],\n",
       "        [ 0.15714386,  0.18969443, -0.06206353, ..., -0.038485  ,\n",
       "         -0.02327139, -0.11775462],\n",
       "        ...,\n",
       "        [-0.09316987,  0.23545504, -0.03873876, ..., -0.02392931,\n",
       "          0.09640294, -0.0650026 ],\n",
       "        [ 0.08903266,  0.12086158, -0.05413786, ...,  0.20740497,\n",
       "          0.25029495, -0.03790988],\n",
       "        [ 0.18049291, -0.10137311,  0.10223415, ...,  0.15622312,\n",
       "          0.03356964, -0.00807157]], dtype=float32),\n",
       " array([-1.4127528e-03,  2.1987204e-05, -4.6421826e-04,  2.0741327e-03,\n",
       "        -2.5712955e-03, -3.4713442e-04,  4.3945570e-04,  3.0047106e-04,\n",
       "         8.3065644e-04, -8.7551649e-05,  5.1270668e-05, -2.0842422e-03,\n",
       "        -2.4037134e-04, -4.8085567e-04,  6.4075465e-04,  4.6212447e-04,\n",
       "         2.6456353e-03, -2.7202815e-04,  8.3146780e-04, -2.6126117e-03,\n",
       "         6.2068219e-05,  1.0610995e-03, -8.9862122e-04, -9.5684337e-04,\n",
       "        -1.5806119e-04, -1.6196553e-03, -1.3880389e-03,  1.1960622e-03,\n",
       "         1.5280795e-03,  4.9785641e-04,  3.7301256e-04,  1.2637059e-03],\n",
       "       dtype=float32),\n",
       " array([[ 0.1789593 ],\n",
       "        [-0.01562216],\n",
       "        [ 0.16827415],\n",
       "        [-0.38511238],\n",
       "        [ 0.30533686],\n",
       "        [ 0.19331232],\n",
       "        [-0.08807516],\n",
       "        [ 0.40961188],\n",
       "        [-0.23397781],\n",
       "        [-0.03743448],\n",
       "        [-0.02760203],\n",
       "        [ 0.36139786],\n",
       "        [ 0.10747728],\n",
       "        [-0.32249188],\n",
       "        [-0.11418239],\n",
       "        [-0.07694664],\n",
       "        [-0.37078944],\n",
       "        [ 0.11283956],\n",
       "        [-0.15330477],\n",
       "        [ 0.3261134 ],\n",
       "        [-0.09263208],\n",
       "        [-0.41037098],\n",
       "        [ 0.30184698],\n",
       "        [ 0.40369657],\n",
       "        [ 0.08273549],\n",
       "        [ 0.38201413],\n",
       "        [ 0.36099678],\n",
       "        [-0.35682493],\n",
       "        [-0.26592386],\n",
       "        [-0.11225544],\n",
       "        [ 0.29372427],\n",
       "        [-0.25407463]], dtype=float32),\n",
       " array([-0.00600961], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cf48899",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=train_images/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "187f8d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3294, 4096)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images=val_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06720a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1912 - acc: 0.7268 - val_loss: 0.1791 - val_acc: 0.7350\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1671 - acc: 0.7456 - val_loss: 0.1435 - val_acc: 0.7842\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1464 - acc: 0.7911 - val_loss: 0.1017 - val_acc: 0.8634\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1311 - acc: 0.8367 - val_loss: 0.1077 - val_acc: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2088df5a0d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_labels, batch_size=64, epochs=4, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "599320f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "094710f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1824 - acc: 0.7274 - val_loss: 0.2026 - val_acc: 0.6776\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1543 - acc: 0.7596 - val_loss: 0.1112 - val_acc: 0.8634\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1340 - acc: 0.8248 - val_loss: 0.1019 - val_acc: 0.8670\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1216 - acc: 0.8512 - val_loss: 0.0905 - val_acc: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2088de1d8e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images,train_labels, batch_size=64, epochs=4, validation_data =(val_images,val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4875d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model2.pickle', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2925d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dash-env)",
   "language": "python",
   "name": "dash-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
